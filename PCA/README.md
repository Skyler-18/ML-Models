# **ğŸ§  Principal Component Analysis (PCA) from Scratch**

In this notebook, I implement Principal Component Analysis (PCA) from scratch using Python, NumPy, and Pandas.

This project serves as a personal mastery checkpoint â€” showcasing my understanding of PCAâ€™s inner workings, dimensionality reduction, and the underlying linear algebra that powers it.

## ğŸ” What This Project Covers
- This notebook walks through PCA step-by-step using first principles, including:
- Standardizing the dataset to ensure fair variance comparison
- Calculating the covariance matrix to capture feature relationships
- Performing eigen decomposition to identify principal components
- Selecting the top eigenvectors based on eigenvalues
- Projecting the original data onto a lower-dimensional space
- Visualizing 3D â†’ 2D projection to show information preservation
- Validating dimensionality reduction by separating classes visually

## ğŸ“š Table of Contents
1. ğŸ“Š Generating Synthetic Dataset
2. ğŸ“‰ Exploring the Dataset
3. âš–ï¸ Standardizing Features
4. ğŸ§® Calculating Covariance Matrix
5. ğŸ§  Eigen Decomposition
6. ğŸ§² Selecting Principal Components
7. ğŸ”„ Projecting Data onto New Axes
8. ğŸ–¼ï¸ Visualizing Dimensionality Reduction
9. âœ… Conclusion

## âš™ï¸ Technologies Used
- NumPy â€“ Matrix operations and eigen decomposition
- Pandas â€“ Data handling and manipulation
- scikit-learn â€“ Only used for StandardScaler (no PCA module used)
- Plotly â€“ 3D and 2D visualization of data and projections

## ğŸ¯ Why This Project?
This is not a library showcase â€” this is a â€œlearn by buildingâ€ initiative. I wanted to understand exactly:

- How PCA identifies directions of maximum variance
- How eigenvectors relate to new feature axes
- What â€œdimensionality reductionâ€ really means behind the scenes
